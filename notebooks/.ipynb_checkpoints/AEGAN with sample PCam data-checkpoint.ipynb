{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Author: Umair Khan\n",
    "- Date: 27/06/20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from os.path import join\n",
    "sys.path.append('..')\n",
    "from models import aegan_model\n",
    "from matplotlib import pyplot\n",
    "from numpy import zeros\n",
    "from numpy import ones\n",
    "from numpy import expand_dims\n",
    "from numpy import mean\n",
    "from numpy.random import randn, uniform\n",
    "from numpy.random import randint\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Conv2DTranspose\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Concatenate\n",
    "from keras.initializers import RandomNormal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import isfile\n",
    "\n",
    "def load_sample_data():\n",
    "    positive_path = \"E:\\project\\gan_pcam\\data\\\\train_sample\\\\positive\"\n",
    "    negative_path = \"E:\\project\\gan_pcam\\data\\\\train_sample\\\\negative\"\n",
    "    positive_images = [f for f in listdir(positive_path) if isfile(join(positive_path, f))]\n",
    "    negative_images = [f for f in listdir(negative_path) if isfile(join(negative_path, f))]\n",
    "    images = []\n",
    "    labels = []\n",
    "    for pos, neg in zip(positive_images, negative_images):\n",
    "        img = Image.open(join(positive_path, pos))\n",
    "        images.append(np.array(img))\n",
    "        labels.append(1)\n",
    "        img = Image.open(join(negative_path, neg))\n",
    "        images.append(np.array(img))\n",
    "        labels.append(0)\n",
    "    return np.array(images), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_real_samples():\n",
    "    # load dataset\n",
    "    #(trainX, trainy), (_, _) = load_data()\n",
    "    trainX, trainy = load_sample_data()\n",
    "    # expand to 3d, e.g. add channels\n",
    "    #X = expand_dims(trainX, axis=-1)\n",
    "    X = trainX\n",
    "    # convert from ints to floats\n",
    "    X = X.astype('float32')\n",
    "    # scale from [0,255] to [-1,1]\n",
    "    X = (X - 127.5) / 127.5\n",
    "    print(X.shape, trainy.shape)\n",
    "    return [X, trainy]\n",
    "\n",
    "# select real samples\n",
    "def generate_real_samples(dataset, n_samples,  noise=False):\n",
    "    # split into images and labels\n",
    "    images, labels = dataset\n",
    "    # choose random instances\n",
    "    ix = randint(0, images.shape[0], n_samples)\n",
    "    # select images and labels\n",
    "    X, labels = images[ix], labels[ix]\n",
    "    # generate class labels\n",
    "    y = ones((n_samples, 1)) if not noise else ones((n_samples, 1)) - uniform(0, 0.1, n_samples).reshape((n_samples, 1))\n",
    "    return [X, labels], y\n",
    "\n",
    "# generate points in latent space as input for the generator\n",
    "def generate_latent_points(latent_dim, n_samples, n_classes=2):\n",
    "    # generate points in the latent space\n",
    "    x_input = uniform(-1, 1, latent_dim * n_samples)\n",
    "    # reshape into a batch of inputs for the network\n",
    "    z_input = x_input.reshape(n_samples, latent_dim)\n",
    "    # generate labels\n",
    "    labels = randint(0, n_classes, n_samples)\n",
    "    return [z_input, labels]\n",
    "\n",
    "# use the generator to generate n fake examples, with class labels\n",
    "def generate_fake_samples(generator, latent_dim, n_samples, noise=False):\n",
    "    z_input, _ = generate_latent_points(latent_dim, n_samples)\n",
    "    images = generator.predict(z_input)\n",
    "    y = zeros((n_samples, 1)) if not noise else zeros((n_samples, 1)) + uniform(0, 0.1, n_samples).reshape((n_samples, 1))\n",
    "    return [images, _], y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Train Encoder-Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 96, 96, 3) (10000,)\n",
      "WARNING:tensorflow:From c:\\users\\320048294\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From c:\\users\\320048294\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\\models\\aegan_model.py:104: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"ac...)`\n",
      "  model = Model(input=image, output=output)\n",
      "..\\models\\aegan_model.py:121: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"ac...)`\n",
      "  model = Model(input=embedding, output=output)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\320048294\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      ">Epoch:1 L1[0.380]]\n",
      ">Epoch:2 L1[0.214]]\n",
      ">Epoch:3 L1[0.173]]\n",
      ">Epoch:4 L1[0.147]]\n",
      ">Epoch:5 L1[0.122]]\n",
      ">Epoch:6 L1[0.108]]\n",
      ">Epoch:7 L1[0.101]6]\n",
      ">Epoch:8 L1[0.096]2]\n",
      ">Epoch:9 L1[0.093]5]\n",
      ">Epoch:10 L1[0.090]]\n",
      ">Saved: .\\AEGAN with sample PCam data\\output\\encoder-decoder\\encoder_decoder_10.png, .\\AEGAN with sample PCam data\\model\\encoder-decoder\\encoder_model_10.h5 and .\\AEGAN with sample PCam data\\model\\encoder-decoder\\decoder_model_10.h5\n",
      ">Epoch:11 L1[0.086]]\n",
      ">Epoch:12 L1[0.084]]\n",
      ">Epoch:13 L1[0.082]]\n",
      ">Epoch:14 L1[0.080]]\n",
      ">Epoch:15 L1[0.080]]\n",
      ">Epoch:16 L1[0.079]]\n",
      ">Epoch:17 L1[0.076]]\n",
      ">Epoch:18 L1[0.075]]\n",
      ">Epoch:19 L1[0.074]]\n",
      ">Epoch:20 L1[0.073]]\n",
      ">Saved: .\\AEGAN with sample PCam data\\output\\encoder-decoder\\encoder_decoder_20.png, .\\AEGAN with sample PCam data\\model\\encoder-decoder\\encoder_model_20.h5 and .\\AEGAN with sample PCam data\\model\\encoder-decoder\\decoder_model_20.h5\n",
      ">Epoch:21 L1[0.072]]\n",
      ">Epoch:22 L1[0.071]]\n",
      ">Epoch:23 L1[0.070]]\n",
      ">Epoch:24 L1[0.069]]\n",
      ">Epoch:25 L1[0.068]]\n",
      ">Epoch:26 L1[0.067]]\n",
      ">Epoch:27 L1[0.067]]\n",
      ">Epoch:28 L1[0.067]]\n",
      ">Epoch:29 L1[0.065]]\n",
      ">Epoch:30 L1[0.065]]\n",
      ">Saved: .\\AEGAN with sample PCam data\\output\\encoder-decoder\\encoder_decoder_30.png, .\\AEGAN with sample PCam data\\model\\encoder-decoder\\encoder_model_30.h5 and .\\AEGAN with sample PCam data\\model\\encoder-decoder\\decoder_model_30.h5\n",
      ">Epoch:31 L1[0.063]]\n",
      ">Epoch:32 L1[0.064]]\n",
      ">Epoch:33 L1[0.063]]\n",
      ">Epoch:34 L1[0.062]]\n",
      ">Epoch:35 L1[0.063]]\n",
      ">Epoch:36 L1[0.061]]\n",
      ">Epoch:37 L1[0.061]]\n",
      ">Epoch:38 L1[0.060]]\n",
      ">Epoch:39 L1[0.060]]\n",
      ">Epoch:40 L1[0.059]]\n",
      ">Saved: .\\AEGAN with sample PCam data\\output\\encoder-decoder\\encoder_decoder_40.png, .\\AEGAN with sample PCam data\\model\\encoder-decoder\\encoder_model_40.h5 and .\\AEGAN with sample PCam data\\model\\encoder-decoder\\decoder_model_40.h5\n",
      ">Epoch:41 L1[0.059]]\n",
      ">Epoch:42 L1[0.059]]\n",
      ">Epoch:43 L1[0.058]]\n",
      ">Epoch:44 L1[0.058]]\n",
      ">Epoch:45 L1[0.058]]\n",
      ">Epoch:46 L1[0.057]]\n",
      ">Epoch:47 L1[0.056]]\n",
      ">Epoch:48 L1[0.056]]\n",
      ">Epoch:49 L1[0.056]]\n",
      ">Epoch:50 L1[0.056]]\n",
      ">Saved: .\\AEGAN with sample PCam data\\output\\encoder-decoder\\encoder_decoder_50.png, .\\AEGAN with sample PCam data\\model\\encoder-decoder\\encoder_model_50.h5 and .\\AEGAN with sample PCam data\\model\\encoder-decoder\\decoder_model_50.h5\n",
      ">Epoch:51 L1[0.056]]\n",
      ">Epoch:52 L1[0.055]]\n",
      ">Epoch:53 L1[0.054]]\n",
      ">Epoch:54 L1[0.054]]\n",
      ">Epoch:55 L1[0.054]]\n",
      ">Epoch:56 L1[0.054]]\n",
      ">Epoch:57 L1[0.054]]\n",
      ">Epoch:58 L1[0.054]]\n",
      ">Epoch:59 L1[0.054]]\n",
      ">Epoch:60 L1[0.053]]\n",
      ">Saved: .\\AEGAN with sample PCam data\\output\\encoder-decoder\\encoder_decoder_60.png, .\\AEGAN with sample PCam data\\model\\encoder-decoder\\encoder_model_60.h5 and .\\AEGAN with sample PCam data\\model\\encoder-decoder\\decoder_model_60.h5\n",
      ">Epoch:61 L1[0.053]]\n",
      ">Epoch:62 L1[0.053]]\n",
      ">Epoch:63 L1[0.053]]\n",
      ">Epoch:64 L1[0.052]]\n",
      ">Epoch:65 L1[0.051]0]\n",
      ">Epoch:66 L1[0.051]3]\n",
      ">Epoch:67 L1[0.052]0]\n",
      ">Epoch:68 L1[0.051]9]\n",
      ">Epoch:69 L1[0.050]8]\n",
      ">Epoch:70 L1[0.051]0]\n",
      ">Saved: .\\AEGAN with sample PCam data\\output\\encoder-decoder\\encoder_decoder_70.png, .\\AEGAN with sample PCam data\\model\\encoder-decoder\\encoder_model_70.h5 and .\\AEGAN with sample PCam data\\model\\encoder-decoder\\decoder_model_70.h5\n",
      ">Epoch:71 L1[0.051]2]\n",
      ">Epoch:72 L1[0.050]3]\n",
      ">Epoch:73 L1[0.050]0]\n",
      ">Epoch:74 L1[0.050]8]\n",
      ">Epoch:75 L1[0.049]9]\n",
      ">Epoch:76 L1[0.050]8]\n",
      ">Epoch:77 L1[0.049]6]\n",
      ">Epoch:78 L1[0.049]5]\n",
      ">Epoch:79 L1[0.048]5]\n",
      ">Epoch:80 L1[0.048]4]\n",
      ">Saved: .\\AEGAN with sample PCam data\\output\\encoder-decoder\\encoder_decoder_80.png, .\\AEGAN with sample PCam data\\model\\encoder-decoder\\encoder_model_80.h5 and .\\AEGAN with sample PCam data\\model\\encoder-decoder\\decoder_model_80.h5\n",
      ">Epoch:81 L1[0.049]9]\n",
      ">Epoch:82 L1[0.047]1]\n",
      ">Epoch:83 L1[0.048]6]\n",
      ">Epoch:84 L1[0.047]8]\n",
      ">Epoch:85 L1[0.047]8]\n",
      ">Epoch:86 L1[0.047]6]\n",
      ">Epoch:87 L1[0.046]6]\n",
      ">Epoch:88 L1[0.047]3]\n",
      ">Epoch:89 L1[0.046]4]\n",
      ">Epoch:90 L1[0.046]5]\n",
      ">Saved: .\\AEGAN with sample PCam data\\output\\encoder-decoder\\encoder_decoder_90.png, .\\AEGAN with sample PCam data\\model\\encoder-decoder\\encoder_model_90.h5 and .\\AEGAN with sample PCam data\\model\\encoder-decoder\\decoder_model_90.h5\n",
      ">Epoch:91 L1[0.047]5]\n",
      ">Epoch:92 L1[0.047]3]\n",
      ">Epoch:93 L1[0.047]3]\n",
      ">Epoch:94 L1[0.046]3]\n",
      ">Epoch:95 L1[0.046]3]\n",
      ">Epoch:96 L1[0.046]2]\n",
      ">Epoch:97 L1[0.045]3]\n",
      ">Epoch:98 L1[0.045]2]\n",
      ">Epoch:99 L1[0.046]3]\n",
      ">Epoch:100 L1[0.045]]\n",
      ">Saved: .\\AEGAN with sample PCam data\\output\\encoder-decoder\\encoder_decoder_100.png, .\\AEGAN with sample PCam data\\model\\encoder-decoder\\encoder_model_100.h5 and .\\AEGAN with sample PCam data\\model\\encoder-decoder\\decoder_model_100.h5\n",
      ">Epoch:101 L1[0.045]]\n",
      ">Epoch:102 L1[0.045]]\n",
      ">Epoch:103 L1[0.045]]\n",
      ">Epoch:104 L1[0.044]]\n",
      ">Epoch:105 L1[0.044]]\n",
      ">Epoch:106 L1[0.044]]\n",
      ">Epoch:107 L1[0.044]]\n",
      ">Epoch:108 L1[0.043]]\n",
      ">Epoch:109 L1[0.044]]\n",
      ">Epoch:110 L1[0.044]]\n",
      ">Saved: .\\AEGAN with sample PCam data\\output\\encoder-decoder\\encoder_decoder_110.png, .\\AEGAN with sample PCam data\\model\\encoder-decoder\\encoder_model_110.h5 and .\\AEGAN with sample PCam data\\model\\encoder-decoder\\decoder_model_110.h5\n",
      ">Epoch:111 L1[0.043]]\n",
      ">Epoch:112 L1[0.044]]\n",
      ">Epoch:113 L1[0.043]]\n",
      ">Epoch:114 L1[0.043]]\n",
      ">Epoch:115 L1[0.043]]\n",
      ">Epoch:116 L1[0.043]]\n",
      ">Epoch:117 L1[0.043]]\n",
      ">Epoch:118 L1[0.043]]\n",
      ">Epoch:119 L1[0.043]]\n",
      ">Epoch:120 L1[0.042]]\n",
      ">Saved: .\\AEGAN with sample PCam data\\output\\encoder-decoder\\encoder_decoder_120.png, .\\AEGAN with sample PCam data\\model\\encoder-decoder\\encoder_model_120.h5 and .\\AEGAN with sample PCam data\\model\\encoder-decoder\\decoder_model_120.h5\n",
      ">Epoch:121 L1[0.042]]\n",
      ">Epoch:122 L1[0.042]]\n",
      ">Epoch:123 L1[0.042]]\n",
      ">Epoch:124 L1[0.042]]\n",
      ">Epoch:125 L1[0.041]]\n",
      ">Epoch:126 L1[0.042]]\n",
      ">Epoch:127 L1[0.041]]\n",
      ">Epoch:128 L1[0.041]]\n",
      ">Epoch:129 L1[0.041]]\n",
      ">Epoch:130 L1[0.042]]\n",
      ">Saved: .\\AEGAN with sample PCam data\\output\\encoder-decoder\\encoder_decoder_130.png, .\\AEGAN with sample PCam data\\model\\encoder-decoder\\encoder_model_130.h5 and .\\AEGAN with sample PCam data\\model\\encoder-decoder\\decoder_model_130.h5\n",
      ">Epoch:131 L1[0.041]]\n",
      ">Epoch:132 L1[0.041]]\n",
      ">Epoch:133 L1[0.041]]\n",
      ">Epoch:134 L1[0.040]]\n",
      ">Epoch:135 L1[0.041]]\n",
      ">Epoch:136 L1[0.040]]\n",
      ">Epoch:137 L1[0.040]]\n",
      ">Epoch:138 L1[0.040]]\n",
      ">Epoch:139 L1[0.040]]\n",
      ">Epoch:140 L1[0.040]]\n",
      ">Saved: .\\AEGAN with sample PCam data\\output\\encoder-decoder\\encoder_decoder_140.png, .\\AEGAN with sample PCam data\\model\\encoder-decoder\\encoder_model_140.h5 and .\\AEGAN with sample PCam data\\model\\encoder-decoder\\decoder_model_140.h5\n",
      ">Epoch:141 L1[0.039]]\n",
      ">Epoch:142 L1[0.039]]\n",
      ">Epoch:143 L1[0.040]]\n",
      ">Epoch:144 L1[0.040]]\n",
      ">Epoch:145 L1[0.039]]\n",
      ">Epoch:146 L1[0.039]]\n",
      ">Epoch:147 L1[0.039]]\n",
      ">Epoch:148 L1[0.039]]\n",
      ">Epoch:149 L1[0.039]]\n",
      ">Epoch:150 L1[0.039]]\n",
      ">Saved: .\\AEGAN with sample PCam data\\output\\encoder-decoder\\encoder_decoder_150.png, .\\AEGAN with sample PCam data\\model\\encoder-decoder\\encoder_model_150.h5 and .\\AEGAN with sample PCam data\\model\\encoder-decoder\\decoder_model_150.h5\n",
      ">Epoch:151 L1[0.039]]\n",
      ">Epoch:152 L1[0.039]]\n",
      ">Epoch:153 L1[0.039]]\n",
      ">Epoch:154 L1[0.038]]\n",
      ">Epoch:155 L1[0.038]]\n",
      ">Epoch:156 L1[0.038]]\n",
      ">Epoch:157 L1[0.038]]\n",
      ">Epoch:158 L1[0.038]]\n",
      ">Epoch:159 L1[0.037]]\n",
      ">Epoch:160 L1[0.038]]\n",
      ">Saved: .\\AEGAN with sample PCam data\\output\\encoder-decoder\\encoder_decoder_160.png, .\\AEGAN with sample PCam data\\model\\encoder-decoder\\encoder_model_160.h5 and .\\AEGAN with sample PCam data\\model\\encoder-decoder\\decoder_model_160.h5\n",
      ">Epoch:161 L1[0.037]]\n",
      ">Epoch:162 L1[0.038]]\n",
      ">Epoch:163 L1[0.037]]\n",
      ">Epoch:164 L1[0.037]]\n",
      ">Epoch:165 L1[0.037]]\n",
      ">Epoch:166 L1[0.037]]\n",
      ">Epoch:167 L1[0.037]]\n",
      ">Epoch:168 L1[0.037]]\n",
      ">Epoch:169 L1[0.037]]\n",
      ">Epoch:170 L1[0.036]]\n",
      ">Saved: .\\AEGAN with sample PCam data\\output\\encoder-decoder\\encoder_decoder_170.png, .\\AEGAN with sample PCam data\\model\\encoder-decoder\\encoder_model_170.h5 and .\\AEGAN with sample PCam data\\model\\encoder-decoder\\decoder_model_170.h5\n",
      ">Epoch:171 L1[0.037]]\n",
      ">Epoch:172 L1[0.036]]\n",
      ">Epoch:173 L1[0.037]]\n",
      ">Epoch:174 L1[0.036]]\n",
      ">Epoch:175 L1[0.036]]\n",
      ">Epoch:176 L1[0.036]]\n",
      ">Epoch:177 L1[0.036]]\n",
      ">Epoch:178 L1[0.036]]\n",
      ">Epoch:179 L1[0.036]]\n",
      ">Epoch:180 L1[0.036]]\n",
      ">Saved: .\\AEGAN with sample PCam data\\output\\encoder-decoder\\encoder_decoder_180.png, .\\AEGAN with sample PCam data\\model\\encoder-decoder\\encoder_model_180.h5 and .\\AEGAN with sample PCam data\\model\\encoder-decoder\\decoder_model_180.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">Epoch:181 L1[0.035]]\n",
      ">Epoch:182 L1[0.036]]\n",
      ">Epoch:183 L1[0.036]]\n",
      ">Epoch:184 L1[0.035]]\n",
      ">Epoch:185 L1[0.035]]\n",
      ">Epoch:186 L1[0.036]]\n",
      ">Epoch:187 L1[0.035]]\n",
      ">Epoch:188 L1[0.035]]\n",
      ">Epoch:189 L1[0.035]]\n",
      ">Epoch:190 L1[0.035]]\n",
      ">Saved: .\\AEGAN with sample PCam data\\output\\encoder-decoder\\encoder_decoder_190.png, .\\AEGAN with sample PCam data\\model\\encoder-decoder\\encoder_model_190.h5 and .\\AEGAN with sample PCam data\\model\\encoder-decoder\\decoder_model_190.h5\n",
      ">Epoch:191 L1[0.035]]\n",
      ">Epoch:192 L1[0.035]]\n",
      ">Epoch:193 L1[0.034]]\n",
      ">Epoch:194 L1[0.035]]\n",
      ">Epoch:195 L1[0.035]]\n",
      ">Epoch:196 L1[0.035]]\n",
      ">Epoch:197 L1[0.035]]\n",
      ">Epoch:198 L1[0.034]]\n",
      ">Epoch:199 L1[0.035]]\n",
      ">Epoch:200 L1[0.034]]\n",
      ">Saved: .\\AEGAN with sample PCam data\\output\\encoder-decoder\\encoder_decoder_200.png, .\\AEGAN with sample PCam data\\model\\encoder-decoder\\encoder_model_200.h5 and .\\AEGAN with sample PCam data\\model\\encoder-decoder\\decoder_model_200.h5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_combined_encoder_decoder(encoder, decoder, opt):\n",
    "    decoder_output = decoder(encoder.output)\n",
    "    model = Model(encoder.input, decoder_output)\n",
    "    model.compile(loss=['mean_absolute_error'], optimizer=opt)\n",
    "    return model\n",
    "\n",
    "# generate samples and save as a plot and save the model\n",
    "def summarize_encoder_decoder_performance(epoch, enc_dec_model, encoder_model, decoder_model, dataset, n_samples=25):\n",
    "    # prepare fake examples\n",
    "    [X_real, _], _ = generate_real_samples(dataset, n_samples)\n",
    "    X = decoder_model.predict(encoder_model.predict(X_real))\n",
    "    # scale from 0 to 255 (as unsigned integer)\n",
    "    X = ((X * 127.5) + 127.5).astype('uint8')\n",
    "    pyplot.figure(figsize=(12, 10))\n",
    "    for i in range(25):\n",
    "        # define subplot\n",
    "        pyplot.subplot(5, 5, 1 + i)\n",
    "        # turn off axis\n",
    "        pyplot.axis('off')\n",
    "        # plot raw pixel data\n",
    "        pyplot.imshow(X[i, :, :, :])\n",
    "    # save plot to file\n",
    "    output = f'.\\\\AEGAN with sample PCam data\\\\output\\\\encoder-decoder\\\\encoder_decoder_{epoch}.png'\n",
    "    pyplot.savefig(output)\n",
    "    pyplot.close()\n",
    "    encoder_filename = f'.\\\\AEGAN with sample PCam data\\\\model\\\\encoder-decoder\\\\encoder_model_{epoch}.h5'\n",
    "    decoder_filename = f'.\\\\AEGAN with sample PCam data\\\\model\\\\encoder-decoder\\\\decoder_model_{epoch}.h5'\n",
    "    encoder_model.save(encoder_filename)\n",
    "    decoder_model.save(decoder_filename)\n",
    "    print('>Saved: %s, %s and %s' % (output, encoder_filename, decoder_filename))\n",
    "\n",
    "\n",
    "def train_encoder_decoder(enc_dec_model, encoder_model, decoder_model, dataset, n_epochs=200, n_batch=64):\n",
    "    bat_per_epo = int(dataset[0].shape[0] / n_batch)\n",
    "    # calculate the number of training iterations\n",
    "    n_steps = bat_per_epo * n_epochs\n",
    "    # calculate the size of half a batch of samples\n",
    "    half_batch = int(n_batch / 2)\n",
    "    # manually enumerate epochs\n",
    "    err_list = []\n",
    "    for i in range(n_steps):\n",
    "        # get randomly selected 'real' samples\n",
    "        [X_real, _], _ = generate_real_samples(dataset, n_batch)\n",
    "        # update discriminator model weights\n",
    "        err = enc_dec_model.train_on_batch(X_real, X_real)\n",
    "        err_list.append(err)\n",
    "        # summarize loss on this batch\n",
    "        print(f'>Step:{i+1} L1[{err:.3f}]\\r', end=\"\")\n",
    "        # evaluate the model performance every 'epoch'\n",
    "        if (i+1) % bat_per_epo == 0:\n",
    "            print(f'>Epoch:{(i+1)//bat_per_epo} L1[{mean(err_list):.3f}]\\r', end=\"\")\n",
    "            print()\n",
    "            err_list = []\n",
    "        if (i+1) % (bat_per_epo * 10) == 0:\n",
    "            summarize_encoder_decoder_performance((i+1)//bat_per_epo, enc_dec_model, encoder_model, decoder_model, dataset)\n",
    "\n",
    "dataset = load_real_samples()\n",
    "encoder_model = aegan_model.get_encoder()\n",
    "decoder_model = aegan_model.get_decoder()\n",
    "opt_RS = Adam(lr=0.00001, beta_1=0.5)\n",
    "enc_dec_model = get_combined_encoder_decoder(encoder_model, decoder_model, opt_RS)\n",
    "\n",
    "train_encoder_decoder(enc_dec_model, encoder_model, decoder_model, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Train Generator Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_gan(g_model, d_model):\n",
    "    # make weights in the discriminator not trainable\n",
    "    d_model.trainable = False\n",
    "    # connect the outputs of the generator to the inputs of the discriminator\n",
    "    gan_output = d_model(g_model.output)\n",
    "    # define gan model as taking noise and label and outputting real/fake and label outputs\n",
    "    model = Model(g_model.input, gan_output)\n",
    "    # compile model\n",
    "    opt = Adam(lr=0.0004, beta_1=0.5)\n",
    "    model.compile(loss=['binary_crossentropy'], optimizer=opt)\n",
    "    return model\n",
    "\n",
    "# generate samples and save as a plot and save the model\n",
    "def summarize_generator_discriminator_performance(epoch, g_model, d_model, dec_model, dataset, latent_dim=99, n_samples=25):\n",
    "    # prepare fake examples\n",
    "    [X, _], _ = generate_fake_samples(g_model, latent_dim, n_samples)\n",
    "    X = dec_model.predict(X)\n",
    "    # scale from 0 to 255 (as unsigned integer)\n",
    "    X = ((X * 127.5) + 127.5).astype('uint8')\n",
    "    pyplot.figure(figsize=(12, 10))\n",
    "    for i in range(25):\n",
    "        # define subplot\n",
    "        pyplot.subplot(5, 5, 1 + i)\n",
    "        # turn off axis\n",
    "        pyplot.axis('off')\n",
    "        # plot raw pixel data\n",
    "        pyplot.imshow(X[i, :, :, :])\n",
    "    # save plot to file\n",
    "    output = f'.\\\\AEGAN with sample PCam data\\\\output\\\\generator_discriminator\\\\generator-discriminator_{epoch}.png'\n",
    "    pyplot.savefig(output)\n",
    "    pyplot.close()\n",
    "    g_filename = f'.\\\\AEGAN with sample PCam data\\\\model\\\\generator_discriminator\\\\g_model_{epoch}.h5'\n",
    "    d_filename = f'.\\\\AEGAN with sample PCam data\\\\model\\\\generator_discriminator\\\\d_model_{epoch}.h5'\n",
    "    g_model.save(g_filename)\n",
    "    d_model.save(d_filename)\n",
    "    print('>Saved: %s, %s and %s' % (output, g_filename, d_filename))\n",
    "\n",
    "def train_generator_discriminator(g_model, d_model, enc_model, dec_model,\n",
    "                                  gan_model, dataset, latent_dim=99, n_epochs=200, n_batch=128):\n",
    "    # calculate the number of batches per training epoch\n",
    "    bat_per_epo = int(dataset[0].shape[0] / n_batch)\n",
    "    # calculate the number of training iterations\n",
    "    n_steps = bat_per_epo * n_epochs\n",
    "    # calculate the size of half a batch of samples\n",
    "    half_batch = int(n_batch / 2)\n",
    "    # manually enumerate epochs\n",
    "    dr_list = []\n",
    "    df_list = []\n",
    "    g_list = []\n",
    "    for i in range(n_steps):\n",
    "        # get randomly selected 'real' samples\n",
    "        [X_real, _], y_real = generate_real_samples(dataset, half_batch, True)\n",
    "        # update discriminator model weights\n",
    "        embedding_real = enc_model.predict(X_real)\n",
    "        dr_loss = d_model.train_on_batch(embedding_real, y_real)\n",
    "        # generate 'fake' examples\n",
    "        [embedding_fake, _], y_fake = generate_fake_samples(g_model, latent_dim, half_batch, True)\n",
    "        # update discriminator model weights\n",
    "        df_loss = d_model.train_on_batch(embedding_fake, y_fake)\n",
    "        # prepare points in latent space as input for the generator\n",
    "        [z_input, _] = generate_latent_points(latent_dim, n_batch)\n",
    "        # create inverted labels for the fake samples\n",
    "        y_gan = ones((n_batch, 1))\n",
    "        # update the generator via the discriminator's error\n",
    "        g_loss = gan_model.train_on_batch(z_input, y_gan)\n",
    "        dr_list.append(dr_loss)\n",
    "        df_list.append(df_loss)\n",
    "        g_list.append(g_loss)\n",
    "        # evaluate the model performance every 'epoch'\n",
    "        print(f'>Step:{i+1} DR[{dr_loss:.3f}], DF[{df_loss:.3f}] GL[{g_loss:.3f}]\\r', end=\"\")\n",
    "        # evaluate the model performance every 'epoch'\n",
    "        if (i+1) % bat_per_epo == 0:\n",
    "            print(f'>Epoch:{(i+1)//bat_per_epo} DR[{mean(dr_list):.3f}], DF[{mean(df_list):.3f}] GL[{mean(g_list):.3f}]\\r', end=\"\")\n",
    "            print()\n",
    "            dr_list = []\n",
    "            df_list = []\n",
    "            g_list = []\n",
    "        if (i+1) % (bat_per_epo * 10) == 0:\n",
    "            summarize_generator_discriminator_performance((i+1)//bat_per_epo, g_model, d_model, dec_model, dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 96, 96, 3) (10000,)\n",
      "WARNING:tensorflow:From c:\\users\\320048294\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From c:\\users\\320048294\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\\models\\aegan_model.py:104: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"ac...)`\n",
      "  model = Model(input=image, output=output)\n",
      "..\\models\\aegan_model.py:121: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"ac...)`\n",
      "  model = Model(input=embedding, output=output)\n",
      "..\\models\\aegan_model.py:53: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"ac...)`\n",
      "  model = Model(input=in_latent, output=output)\n",
      "..\\models\\aegan_model.py:86: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"re...)`\n",
      "  model = Model(input=embedding, output=output)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\320048294\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\320048294\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected input_4 to have shape (48, 48, 128) but got array with shape (64, 64, 128)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-137791ab540f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m train_generator_discriminator(g_model, d_model, enc_model, dec_model,\n\u001b[1;32m---> 20\u001b[1;33m                                   gan_model, dataset, latent_dim, n_epochs=400, n_batch=64)\n\u001b[0m",
      "\u001b[1;32m<ipython-input-4-3d96f19e507e>\u001b[0m in \u001b[0;36mtrain_generator_discriminator\u001b[1;34m(g_model, d_model, enc_model, dec_model, gan_model, dataset, latent_dim, n_epochs, n_batch)\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0membedding_fake\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_fake\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_fake_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlatent_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhalf_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[1;31m# update discriminator model weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m         \u001b[0mdf_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0md_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membedding_fake\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_fake\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m         \u001b[1;31m# prepare points in latent space as input for the generator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mz_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_latent_points\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlatent_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\320048294\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1209\u001b[0m             \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1210\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1211\u001b[1;33m             class_weight=class_weight)\n\u001b[0m\u001b[0;32m   1212\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_uses_dynamic_learning_phase\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1213\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1.\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\320048294\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 751\u001b[1;33m             exception_prefix='input')\n\u001b[0m\u001b[0;32m    752\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\320048294\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    136\u001b[0m                             \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m                             str(data_shape))\n\u001b[0m\u001b[0;32m    139\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected input_4 to have shape (48, 48, 128) but got array with shape (64, 64, 128)"
     ]
    }
   ],
   "source": [
    "dataset = load_real_samples()\n",
    "latent_dim = 112\n",
    "enc_model = aegan_model.get_encoder()\n",
    "enc_model.load_weights('.\\\\AEGAN with sample PCam data\\\\model\\\\encoder-decoder\\\\encoder_model_200.h5')\n",
    "dec_model = aegan_model.get_decoder()\n",
    "dec_model.load_weights('.\\\\AEGAN with sample PCam data\\\\model\\\\encoder-decoder\\\\decoder_model_200.h5')\n",
    "\n",
    "g_model = aegan_model.get_embedding_generator(dropout=True)\n",
    "\n",
    "d_model = aegan_model.get_embedding_discriminator(dropout=True)\n",
    "opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "d_model.compile(loss=['binary_crossentropy'], optimizer=opt)\n",
    "\n",
    "#g_model.load_weights('.\\\\AEGAN with sample PCam data\\\\model\\\\generator_discriminator\\\\g_model_300.h5')\n",
    "#d_model.load_weights('.\\\\AEGAN with sample PCam data\\\\model\\\\generator_discriminator\\\\d_model_300.h5')\n",
    "\n",
    "gan_model = define_gan(g_model, d_model)\n",
    "\n",
    "train_generator_discriminator(g_model, d_model, enc_model, dec_model,\n",
    "                                  gan_model, dataset, latent_dim, n_epochs=400, n_batch=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
